# S3 Support for Parser

This document describes how to use S3 (or S3-compatible services like MinIO) to store and parse Dota 2 replay files.

## Features

- **S3 Integration**: Download replay files directly from S3 buckets
- **MinIO Support**: Works with MinIO for local development and testing
- **Backward Compatible**: Still supports HTTP/HTTPS URLs via curl
- **Automatic Format Detection**: Handles both `.dem` and `.dem.bz2` files

## Configuration

The parser uses environment variables for S3 configuration:

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `S3_ENDPOINT` | Custom S3 endpoint URL (for MinIO/custom S3) | None | No* |
| `S3_REGION` | AWS region | `us-east-1` | No |
| `S3_ACCESS_KEY` | AWS access key ID | None | No** |
| `S3_SECRET_KEY` | AWS secret access key | None | No** |
| `S3_PATH_STYLE_ACCESS` | Use path-style access (required for MinIO) | `false` | No |

\* Required for MinIO or non-AWS S3 services  
\** Uses AWS default credentials if not provided

## Usage

### S3 URL Format

Use the following format for S3 URLs:

```
s3://bucket-name/path/to/replay.dem
s3://bucket-name/path/to/replay.dem.bz2
```

### API Request

Make a request to the `/blob` endpoint with the S3 URL:

```bash
curl "http://localhost:5600/blob?replay_url=s3://bucket-name/replays/7503212404.dem"
```

### HTTP URLs (Traditional)

HTTP/HTTPS URLs still work as before:

```bash
curl "http://localhost:5600/blob?replay_url=https://example.com/replay.dem.bz2"
```

## Local Development with MinIO

### Prerequisites

- Docker and Docker Compose
- Python 3 with boto3 (for Python test script): `pip install boto3`
- OR MinIO Client (mc) for bash test script

### Quick Start

1. **Start the services** (including MinIO):
   ```bash
   docker-compose -f docker-compose.dev.yml up
   ```

2. **Run the test script**:
   
   Option A - Bash script (recommended):
   ```bash
   bash scripts/test/test_s3.sh
   ```
   
   Option B - Python script:
   ```bash
   python3 scripts/test/test_s3.py
   ```

3. **Access MinIO Console** (optional):
   - URL: http://localhost:9001
   - Username: `minioadmin`
   - Password: `minioadmin`

### Manual Testing

1. **Install MinIO Client** (if not already installed):
   ```bash
   # macOS
   brew install minio/stable/mc
   
   # Linux
   curl -o /tmp/mc https://dl.min.io/client/mc/release/linux-amd64/mc
   chmod +x /tmp/mc
   sudo mv /tmp/mc /usr/local/bin/mc
   ```

2. **Configure MinIO connection**:
   ```bash
   mc alias set local http://localhost:9000 minioadmin minioadmin
   ```

3. **Create bucket and upload file**:
   ```bash
   mc mb local/dota-replays
   mc cp test-data/7503212404.dem local/dota-replays/replays/7503212404.dem
   ```

4. **Test the parser**:
   ```bash
   curl "http://localhost:5600/blob?replay_url=s3://dota-replays/replays/7503212404.dem"
   ```

## Production Deployment with AWS S3

### Setup

1. **Create an S3 bucket**:
   ```bash
   aws s3 mb s3://your-dota-replays
   ```

2. **Upload replay files**:
   ```bash
   aws s3 cp replay.dem s3://your-dota-replays/replays/
   ```

3. **Configure the parser** with environment variables:
   ```bash
   export S3_REGION=us-east-1
   export S3_ACCESS_KEY=your-access-key
   export S3_SECRET_KEY=your-secret-key
   ```
   
   Or use IAM roles (recommended for EC2/ECS):
   - Attach an IAM role with S3 read permissions to your instance
   - The parser will automatically use the instance credentials

4. **Start the parser**:
   ```bash
   docker-compose up
   ```

### IAM Policy Example

Grant read-only access to your replay bucket:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-dota-replays",
        "arn:aws:s3:::your-dota-replays/*"
      ]
    }
  ]
}
```

## Architecture

### How It Works

1. **Request Flow**:
   ```
   Client Request → BlobHandler → S3Service → Download from S3
                                           ↓
   Response ← JSON Blob ← Parser ← Decompress (if .bz2)
   ```

2. **S3Service**: 
   - Handles S3 authentication and configuration
   - Downloads files from S3 buckets
   - Supports both AWS S3 and S3-compatible services

3. **BlobHandler**:
   - Detects S3 URLs (starts with `s3://`)
   - Routes to appropriate handler (S3 or HTTP)
   - Manages decompression pipeline

## Troubleshooting

### MinIO Connection Issues

**Problem**: "S3 download error: Unable to execute HTTP request"

**Solution**: 
- Check if MinIO is running: `docker ps | grep minio`
- Verify endpoint: `S3_ENDPOINT=http://minio:9000` (inside Docker network)
- For testing outside Docker: `S3_ENDPOINT=http://localhost:9000`

### Authentication Errors

**Problem**: "Access Denied" or "Invalid credentials"

**Solution**:
- Verify `S3_ACCESS_KEY` and `S3_SECRET_KEY` are set correctly
- For AWS, check IAM permissions
- For MinIO, use `minioadmin/minioadmin` (default)

### Path Style Access

**Problem**: "The specified bucket does not exist"

**Solution**:
- Set `S3_PATH_STYLE_ACCESS=true` for MinIO
- MinIO requires path-style URLs: `http://endpoint/bucket/key`
- AWS S3 uses virtual-hosted style by default

## Testing

### Automated Tests

Run the included test scripts:

```bash
# Bash version (auto-installs mc if needed)
bash scripts/test/test_s3.sh

# Python version (requires boto3)
pip install boto3
python3 scripts/test/test_s3.py
```

### Manual Tests

1. **Test MinIO connectivity**:
   ```bash
   curl http://localhost:9000/minio/health/live
   ```

2. **Test parser health**:
   ```bash
   curl http://localhost:5600/healthz
   ```

3. **Test S3 parsing**:
   ```bash
   curl "http://localhost:5600/blob?replay_url=s3://bucket/key"
   ```

## Performance Considerations

- S3 downloads are streamed directly to the parser
- No temporary files are created on disk
- Decompression happens in-memory via pipeline
- Suitable for large replay files (100MB+)

## Security Best Practices

1. **Use IAM Roles** when possible (EC2/ECS)
2. **Restrict S3 permissions** to read-only
3. **Use VPC endpoints** for AWS S3 in production
4. **Enable encryption** at rest for S3 buckets
5. **Rotate credentials** regularly if using access keys

## Migration from HTTP to S3

1. **Upload existing replays to S3**:
   ```bash
   aws s3 sync ./replays/ s3://your-bucket/replays/
   ```

2. **Update your application** to use S3 URLs:
   ```javascript
   // Before
   const url = `https://example.com/replays/${matchId}.dem`;
   
   // After
   const url = `s3://your-bucket/replays/${matchId}.dem`;
   ```

3. **No parser changes needed** - the parser automatically detects URL type

## Additional Resources

- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)
- [AWS SDK for Java S3 Guide](https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/examples-s3.html)

